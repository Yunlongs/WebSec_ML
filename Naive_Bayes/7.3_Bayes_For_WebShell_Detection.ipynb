{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web安全之机器学习\n",
    "## 第7章朴素贝叶斯算法\n",
    "### 7.4 示例：检测WebShell（一）\n",
    "\n",
    "#### 1. 数据搜集和数据清洗\n",
    "我们将搜集到的WebShell作为黑样本，wordpress源码作为白样本。将一个PHP文件作为一个字符串处理，基于2-gram切割，形成基于2-gram的词汇表。\n",
    "（1）将WebShell的每一个样本文件，转化为一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_one_file(filename):\n",
    "    string = \"\"\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            string += line.strip()\n",
    "    return string\n",
    "\n",
    "def convert_to_string(rootdir):\n",
    "    file_list = os.listdir(rootdir)\n",
    "    x = []\n",
    "    for file in file_list:\n",
    "        if file.endswith('.php'):\n",
    "            file_path = os.path.join(rootdir,file)\n",
    "            x.append(get_one_file(file_path))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）形成基于2-gram算法称重的全局词汇表。\n",
    "这里仍然可以使用CountVectorizer用来向量化，但是需要特别说明3个参数：\n",
    "\n",
    "- **ngram-range(min_n,max_n)：**提取的不同ngram的n值的上下界，设置成（2，2）表示基于2-gram\n",
    "- **decode-errors{‘strict’, ‘ignore’, ‘replace’}：**设置成ignore，表明忽略异常字符的影响；strict将会引起报错。\n",
    "- **token_pattern：**设置成r'\\bw+b'，表明按照单词切割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def webshell_vectorize():\n",
    "    wb_vectorizer = CountVectorizer(ngram_range=(2,2),decode_error='ignore',token_pattern=r'\\bw+\\b',min_df=1)\n",
    "    wb_file_list = convert_to_string(\"../data/PHP-WEBSHELL/dama/\")\n",
    "    x1 = wb_vectorizer.fit_transform(wb_file_list).toarray()\n",
    "    vocabulary = wb_vectorizer.vocabulary_\n",
    "    return x1,vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.特征化\n",
    "使用webshell样本生成的词汇表vocabulary，将wordpress样本特征化。其中**最重要的是设置CountVectorizer函数的vocabulary，这样采用使用黑样本的词汇表进行向量化**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature():\n",
    "    x1,vocabulary = webshell_vectorize()\n",
    "    wp_vectorizer = CountVectorizer(ngram_range=(2,2),decode_error='ignore',token_pattern=r'\\bw+\\b',min_df=1,vocabulary=vocabulary)\n",
    "    wp_file_list = convert_to_string(\"../data/wordpress\")\n",
    "    x2 = wp_vectorizer.fit_transform(wp_file_list).toarray()\n",
    "\n",
    "    y1,y2 = np.ones((x1.shape[0],1)),np.zeros((x2.shape[0],1))\n",
    "\n",
    "    X = np.vstack((x1,x2))\n",
    "    Y = np.vstack((y1,y2)).ravel()\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练样本和效果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score= [ 0.90384615  0.86538462  0.76470588]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X,Y = get_feature()\n",
    "    clf = GaussianNB()\n",
    "    score = cross_val_score(clf,X,Y,cv=3)\n",
    "\n",
    "    print(\"score=\",score)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 示例：检测WebShell（二）\n",
    "#### 1.数据搜集和数据清洗\n",
    "我们可以把WebShell看作为远程管理的一系列功能函数的调用，所以我们可以针对函数调用建立特征。针对黑样本集合，以1-gram算法形成全局的词汇表，切割方法为提取函数调用的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wbshell_vectorize_new():\n",
    "    wbshell_vectorizer = CountVectorizer(ngram_range=(1,1),decode_error='ignore',token_pattern=r'\\b\\w+\\b\\(|\\'\\w+\\'',min_df=1)\n",
    "    wb_file_list = convert_to_string(\"../data/PHP-WEBSHELL/dama/\")\n",
    "    x1 = wbshell_vectorizer.fit_transform(wb_file_list).toarray()\n",
    "    vocabulary = wbshell_vectorizer.vocabulary_\n",
    "    return x1,vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.特征化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_new():\n",
    "    x1,vocabulary = wbshell_vectorize_new()\n",
    "    wp_vectorizer = CountVectorizer(ngram_range=(1,1),decode_error='ignore',token_pattern=r'\\b\\w+\\b\\(|\\'\\w+\\'',min_df=1,vocabulary=vocabulary)\n",
    "    wp_file_list = convert_to_string(\"../data/wordpress\")\n",
    "    x2 = wp_vectorizer.fit_transform(wp_fi9le_list).toarray()\n",
    "    y1, y2 = np.ones((x1.shape[0], 1)), np.zeros((x2.shape[0], 1))\n",
    "\n",
    "    X = np.vstack((x1, x2))\n",
    "    Y = np.vstack((y1, y2)).ravel()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score= [ 0.71153846  0.96153846  0.70588235]\n"
     ]
    }
   ],
   "source": [
    "def main_1():\n",
    "    X,Y = get_feature_new()\n",
    "    clf = GaussianNB()\n",
    "    score = cross_val_score(clf,X,Y,cv=3)\n",
    "\n",
    "    print(\"score=\",score)\n",
    "main_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
